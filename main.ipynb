{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7872\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datetime import datetime\n",
    "import gradio as gr\n",
    "from docx import Document  # For reading .docx files\n",
    "import PyPDF2  # For reading .pdf files\n",
    "\n",
    "# Load tokenizer and model for resume analysis\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set pad_token to eos_token if it is not defined\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define token limit for the model (this depends on the model you're using, e.g., GPT-2 has 1024 tokens)\n",
    "MAX_TOKENS = tokenizer.model_max_length\n",
    "\n",
    "# Function to safely tokenize and truncate long input\n",
    "def safe_tokenize(text, max_length=MAX_TOKENS):\n",
    "    # Tokenize the input text and truncate it to fit within the max token length\n",
    "    tokens = tokenizer(text, truncation=True, max_length=max_length, padding='max_length', return_tensors=\"pt\")\n",
    "    return tokens\n",
    "\n",
    "# Define the CSV file for storing job application history\n",
    "history_file = \"job_applications.csv\"\n",
    "\n",
    "# Function to extract text from uploaded resume (supports .docx and .pdf)\n",
    "def extract_resume_text(file):\n",
    "    if file.name.endswith(\".docx\"):\n",
    "        doc = Document(file.name)\n",
    "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    elif file.name.endswith(\".pdf\"):\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "    else:\n",
    "        return \"Unsupported file format. Please upload a .docx or .pdf file.\"\n",
    "\n",
    "# Function to analyze resume against job description\n",
    "def analyze_resume(resume, job_desc):\n",
    "    prompt = f\"Analyze the following resume against the given job description.\\n\\nResume:\\n{resume}\\n\\nJob Description:\\n{job_desc}\"\n",
    "    \n",
    "    # Tokenize and truncate both resume and job description\n",
    "    tokens = safe_tokenize(prompt)\n",
    "\n",
    "    # Check if the tokenized input has any content\n",
    "    if tokens.input_ids.shape[1] == 0:\n",
    "        return \"Error: The tokenized input is empty. Please check the input content.\"\n",
    "\n",
    "    # Generate analysis using the model\n",
    "    try:\n",
    "        response = model.generate(\n",
    "            input_ids=tokens.input_ids,\n",
    "            attention_mask=tokens.attention_mask,\n",
    "            max_new_tokens=100,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Ensure that the response has valid content\n",
    "        if response is None or len(response) == 0:\n",
    "            return \"Error: The model failed to generate a response.\"\n",
    "        \n",
    "        # Decode and return analysis\n",
    "        analysis_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "        return analysis_text\n",
    "    except IndexError:\n",
    "        return \"Error: The model's response is out of range.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Function to save job application data to CSV\n",
    "def save_application_data(job_title, job_desc, rating, suggestions):\n",
    "    data = {\n",
    "        \"Job Title\": job_title,\n",
    "        \"Job Description\": job_desc,\n",
    "        \"Rating\": rating,\n",
    "        \"Suggestions\": suggestions,\n",
    "        \"Date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    df = pd.DataFrame([data])\n",
    "    \n",
    "    if not pd.io.common.file_exists(history_file):\n",
    "        df.to_csv(history_file, index=False)\n",
    "    else:\n",
    "        df.to_csv(history_file, mode='a', header=False, index=False)\n",
    "\n",
    "# Function to view job application history\n",
    "def view_history():\n",
    "    if pd.io.common.file_exists(history_file):\n",
    "        return pd.read_csv(history_file)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"Job Title\", \"Job Description\", \"Rating\", \"Suggestions\", \"Date\"])\n",
    "\n",
    "# Define Gradio interface for the application\n",
    "def main(resume_file, job_desc, job_title):\n",
    "    resume = extract_resume_text(resume_file)\n",
    "    analysis = analyze_resume(resume, job_desc)\n",
    "    \n",
    "    # Save data (placeholder for actual rating and suggestions in the analysis response)\n",
    "    rating = 80  # Placeholder for rating\n",
    "    suggestions = \"Improve formatting and highlight relevant experience.\"  # Placeholder for suggestions\n",
    "    \n",
    "    save_application_data(job_title, job_desc, rating, suggestions)\n",
    "    \n",
    "    return f\"Resume Analysis:\\n{analysis}\", suggestions\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"AI Resume Analysis Assistant\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        resume_file = gr.File(label=\"Upload your Resume (.docx or .pdf)\")\n",
    "        job_desc = gr.Textbox(label=\"Paste the Job Description\", lines=10)\n",
    "        job_title = gr.Textbox(label=\"Job Title\")\n",
    "        \n",
    "    with gr.Row():\n",
    "        submit = gr.Button(\"Analyze Resume\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        analysis_output = gr.Textbox(label=\"Analysis Result\", interactive=False)\n",
    "        suggestions_output = gr.Textbox(label=\"Suggestions\", interactive=False)\n",
    "    \n",
    "    with gr.Row():\n",
    "        history_button = gr.Button(\"View Application History\")\n",
    "        history_output = gr.Dataframe(label=\"Application History\", interactive=False)\n",
    "    \n",
    "    # Fix: Match the number of outputs with what main() returns\n",
    "    submit.click(\n",
    "        fn=main,\n",
    "        inputs=[resume_file, job_desc, job_title],\n",
    "        outputs=[analysis_output, suggestions_output]\n",
    "    )\n",
    "    history_button.click(fn=view_history, outputs=[history_output])\n",
    "\n",
    "# Launch the app\n",
    "app.launch(pwa=True, share=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
