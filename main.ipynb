{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://280f989e95b06eed15.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://280f989e95b06eed15.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 2042, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1589, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/utils.py\", line 883, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/df/npmhf4fs0qb8cnwm2kmptxk00000gn/T/ipykernel_45361/2296764282.py\", line 79, in main\n",
      "    rating, suggestions = analyze_resume(resume, job_desc)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/df/npmhf4fs0qb8cnwm2kmptxk00000gn/T/ipykernel_45361/711787485.py\", line 51, in analyze_resume\n",
      "    response = model.generate(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/generation/utils.py\", line 2047, in generate\n",
      "    result = self._sample(\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/generation/utils.py\", line 3007, in _sample\n",
      "    outputs = self(**model_inputs, return_dict=True)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1316, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "                          ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1029, in forward\n",
      "    position_embeds = self.wpe(position_ids)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/sparse.py\", line 164, in forward\n",
      "    return F.embedding(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py\", line 2267, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "IndexError: index out of range in self\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datetime import datetime\n",
    "import gradio as gr\n",
    "from docx import Document  # For reading .docx files\n",
    "import PyPDF2  # For reading .pdf files\n",
    "\n",
    "# Load tokenizer and model for resume analysis\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set pad_token to eos_token if it is not defined\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define token limit for the model (this depends on the model you're using, e.g., GPT-2 has 1024 tokens)\n",
    "MAX_TOKENS = tokenizer.model_max_length\n",
    "\n",
    "# Function to safely tokenize and truncate long input\n",
    "def safe_tokenize(text, max_length=MAX_TOKENS):\n",
    "    # Tokenize the input text and truncate it to fit within the max token length\n",
    "    tokens = tokenizer(text, truncation=True, max_length=max_length, padding='max_length', return_tensors=\"pt\")\n",
    "    return tokens\n",
    "\n",
    "# Define the CSV file for storing job application history\n",
    "history_file = \"job_applications.csv\"\n",
    "\n",
    "# Function to extract text from uploaded resume (supports .docx and .pdf)\n",
    "def extract_resume_text(file):\n",
    "    if file.name.endswith(\".docx\"):\n",
    "        doc = Document(file.name)\n",
    "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    elif file.name.endswith(\".pdf\"):\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "    else:\n",
    "        return \"Unsupported file format. Please upload a .docx or .pdf file.\"\n",
    "\n",
    "# Function to analyze resume against job description\n",
    "def analyze_resume(resume, job_desc):\n",
    "    prompt = f\"Analyze the following resume against the given job description.\\n\\nResume:\\n{resume}\\n\\nJob Description:\\n{job_desc}\"\n",
    "    \n",
    "    # Tokenize and truncate both resume and job description\n",
    "    tokens = safe_tokenize(prompt)\n",
    "\n",
    "    # Generate analysis using the model\n",
    "    response = model.generate(\n",
    "        input_ids=tokens.input_ids,\n",
    "        attention_mask=tokens.attention_mask,\n",
    "        max_new_tokens=100,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode and return analysis\n",
    "    analysis_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "    return analysis_text\n",
    "\n",
    "# Function to save job application data to CSV\n",
    "def save_application_data(job_title, job_desc, rating, suggestions):\n",
    "    data = {\n",
    "        \"Job Title\": job_title,\n",
    "        \"Job Description\": job_desc,\n",
    "        \"Rating\": rating,\n",
    "        \"Suggestions\": suggestions,\n",
    "        \"Date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    df = pd.DataFrame([data])\n",
    "    \n",
    "    if not pd.io.common.file_exists(history_file):\n",
    "        df.to_csv(history_file, index=False)\n",
    "    else:\n",
    "        df.to_csv(history_file, mode='a', header=False, index=False)\n",
    "\n",
    "# Function to view job application history\n",
    "def view_history():\n",
    "    if pd.io.common.file_exists(history_file):\n",
    "        return pd.read_csv(history_file)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"Job Title\", \"Job Description\", \"Rating\", \"Suggestions\", \"Date\"])\n",
    "\n",
    "# Define Gradio interface for the application\n",
    "def main(resume_file, job_desc, job_title):\n",
    "    resume = extract_resume_text(resume_file)\n",
    "    analysis = analyze_resume(resume, job_desc)\n",
    "    \n",
    "    # Save data (placeholder for actual rating and suggestions in the analysis response)\n",
    "    rating = 80  # Placeholder for rating\n",
    "    suggestions = \"Improve formatting and highlight relevant experience.\"  # Placeholder for suggestions\n",
    "    \n",
    "    save_application_data(job_title, job_desc, rating, suggestions)\n",
    "    \n",
    "    return f\"Resume Analysis:\\n{analysis}\", suggestions\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"# AI Resume Analysis Assistant\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        resume_file = gr.File(label=\"Upload your Resume (.docx or .pdf)\")\n",
    "        job_desc = gr.Textbox(label=\"Paste the Job Description\", lines=10)\n",
    "        job_title = gr.Textbox(label=\"Job Title\")\n",
    "        \n",
    "    with gr.Row():\n",
    "        submit = gr.Button(\"Analyze Resume\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        result = gr.Textbox(label=\"Analysis Result\", interactive=False)\n",
    "        suggestions_output = gr.Textbox(label=\"Suggestions\", lines=5, interactive=False)\n",
    "    \n",
    "    with gr.Row():\n",
    "        history_button = gr.Button(\"View Application History\")\n",
    "        history_output = gr.Dataframe(label=\"Application History\", interactive=False)\n",
    "    \n",
    "    submit.click(main, inputs=[resume_file, job_desc, job_title], outputs=[result, suggestions_output])\n",
    "    history_button.click(view_history, outputs=history_output)\n",
    "\n",
    "# Launch the app\n",
    "app.launch(pwa=True, share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
